[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Currently, I am a student at Istanbul Technical University. I like to work on data science and related fields. In this blog I will be writing about mostly machine learning, computer science, and mathematics."
  },
  {
    "objectID": "posts/vgg/index.html",
    "href": "posts/vgg/index.html",
    "title": "VGG Paper Implementation from Scratch",
    "section": "",
    "text": "Introduction\nIn this article I will be guiding you through the implementation of VGG paper. VGG paper is published in 2015. Title of the paper is “Very Deep Convolutional Networks for Large-Scale Image Recognition”. I agree, quite a mouthful but the proposed models managed to place second place in Classification and first place in Localization for ImageNet 2014 Challange. Mainly, authors investigate the following:\n\nUse of deeper networks (Convolutional that is)\nSmall filter sizes (3x3 and 1x1)\nSystematically decreasing feature size and increasing filter count\n\nArchitectures presented in the paper consist of Conv, MaxPool, Fully-Connected layers and ReLU activation (apart from softmax for multi-class classification). Nowadays, BatchNorm is in wide use but it wasn’t on the map at the time of this paper (although one of the architectures utilized LocalResponseNormalization but it is reported to have no benefit). Similarly Dropout layers are not used as well. Layer configurations can be summarized as:\n\nReLU Activation after every parametrized layer\nMaxPool layers (2x2) window with stride 2\nCommon fully-connected layers accross the proposed architectures\nConvolutional layers with (3x3) and (1x1) filters with “same” padding. Filter count ranges from 64 to 512 and incremented by doubling\n\n\nArchitecture Summary\n\n\n\n\n\n\n\n\n\n\nA\nA-LRN\nB\nC\nD\nE\n\n\n\n\n11 layers\n11 layers\n13 layers\n16 layers\n16 layers\n19 layers\n\n\nconv3-64\nconv3-64\nconv3-64\nconv3-64\nconv3-64\nconv3-64\n\n\n\nLRN\nconv3-64\nconv3-64\nconv3-64\nconv3-64\n\n\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\n\n\nconv3-128\nconv3-128\nconv3-128\nconv3-128\nconv3-128\nconv3-128\n\n\n\n\nconv3-128\nconv3-128\nconv3-128\nconv3-128\n\n\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\n\n\nconv3-256\nconv3-256\nconv3-256\nconv3-256\nconv3-256\nconv3-256\n\n\nconv3-256\nconv3-256\nconv3-256\nconv3-256\nconv3-256\nconv3-256\n\n\n\n\n\nconv1-256\nconv3-256\nconv3-256\n\n\n\n\n\n\n\nconv3-256\n\n\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\n\n\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\n\n\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\n\n\n\n\n\nconv1-512\nconv3-512\nconv3-512\n\n\n\n\n\n\n\nconv3-512\n\n\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\n\n\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\n\n\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\nconv3-512\n\n\n\n\n\nconv1-512\nconv3-512\nconv3-512\n\n\n\n\n\n\n\nconv3-512\n\n\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\nmaxpool\n\n\nFC-4096\nFC-4096\nFC-4096\nFC-4096\nFC-4096\nFC-4096\n\n\nFC-4096\nFC-4096\nFC-4096\nFC-4096\nFC-4096\nFC-4096\n\n\nFC-1000\nFC-1000\nFC-1000\nFC-1000\nFC-1000\nFC-1000\n\n\nsoft-max\nsoft-max\nsoft-max\nsoft-max\nsoft-max\nsoft-max\n\n\n\n\n\nImplementation\nIn this tutorial I will be usign PyTorch. It is a flexible library for deep learning and tensor operations (like numpy) for gpu and cpu. “nn” module is a framework for defining neural networks with predefined layers (so we don’t have to write our Conv Layer for example). I will be using the Caltech256 dataset from the torchvision datasets module. Dataloader wraps an iterable around the dataset so that it would be easier to train.\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\n\nimport PIL\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os\nimport glob\nfrom collections import OrderedDict\n\nBy instantiating Caltech256, Caltech256 Dataset will be downloaded unzipped and loaded with PIL library. ToTensor transforms theses PIL Images to Torch Tensors.\n\n# Caltech256 has both RGB and Grayscale images\ndef gray_to_rgb(image: PIL.Image):\n    return image.convert(\"RGB\")\n\ndataset = datasets.Caltech256(\"data/\",\n                                download=True,\n                                transform=transforms.Compose([\n                                    gray_to_rgb,\n                                    transforms.ToTensor(),\n                                    transforms.Resize((224, 224), antialias=False),\n                                ]))\ndataloader = DataLoader(dataset,\n                        batch_size=16,\n                        shuffle=True)\n\nFiles already downloaded and verified\n\n\n\nlabel_to_category = list(enumerate([os.path.basename(os.path.normpath(dir).split(\".\")[1]) \n for dir in glob.glob(\"data/caltech256/256_ObjectCategories/*/\")]))\n\ndef map_label_to_category(label):\n    return label_to_category[label][1]\n\nmap_label_to_category(1)\n\n'american-flag'\n\n\n\nbatch = next(iter(dataloader))\n\nplt.figure(figsize=(15, 10))\nfor index, (images, labels) in enumerate(zip(batch[0], batch[1])):\n    plt.subplot(4, 4, index + 1)\n    plt.imshow(images.permute(1, 2, 0), )\n    plt.title(map_label_to_category(labels.item()))\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nNow that we have out dataset ready, let’s start implementing the paper\n\n# Inherit from nn.Module for compatibility with the nn API\nclass VGG(nn.Module):\n\n    # define architecture parameters \n    # tuple -&gt; Conv2d(in_channels, out_channels, kernel_size)\n    conv_architectures = dict(\n        A=[(3, 64, 3), \"max\", (64, 128, 3), \"max\", (128, 256, 3), (256, 256, 3), \"max\", (256, 512, 3), (512, 512, 3), \"max\", (512, 512, 3), (512, 512, 3), \"max\"],\n        A_LRN=[(3, 64, 3), \"lrn\", \"max\", (64, 128, 3), \"max\", (128, 256, 3), (256, 256, 3), \"max\", (256, 512, 3), (512, 512, 3), \"max\", (512, 512, 3), (512, 512, 3), \"max\"],\n        B=[(3, 64, 3), (64, 64, 3), \"max\", (64, 128, 3), (128, 128, 3), \"max\", (128, 256, 3), (256, 256, 3), \"max\", (256, 512, 3), (512, 512, 3), \"max\", (512, 512, 3), (512, 512, 3), \"max\"],\n        C=[(3, 64, 3), (64, 64, 3), \"max\", (64, 128, 3), (128, 128, 3), \"max\", (128, 256, 3), (256, 256, 3), (256, 256, 1), \"max\", (256, 512, 3), (512, 512, 3), (512, 512, 1), \"max\", (512, 512, 3), (512, 512, 3), (512, 512, 1), \"max\"],\n        D=[(3, 64, 3), (64, 64, 3), \"max\", (64, 128, 3), (128, 128, 3), \"max\", (128, 256, 3), (256, 256, 3), (256, 256, 3), \"max\", (256, 512, 3), (512, 512, 3), (512, 512, 3), \"max\", (512, 512, 3), (512, 512, 3), (512, 512, 3), \"max\"],\n        E=[(3, 64, 3), (64, 64, 3), \"max\", (64, 128, 3), (128, 128, 3), \"max\", (128, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), \"max\", (256, 512, 3), (512, 512, 3), (512, 512, 3), (512, 512, 3), \"max\", (512, 512, 3), (512, 512, 3), (512, 512, 3), (512, 512, 3), \"max\"],\n    )\n\n    def __init__(self, architecture_identifier=\"A\", batch_norm=False):\n        super().__init__()\n        self.fully_connected = nn.Sequential(\n            nn.Flatten(),\n            nn.LazyLinear(4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 1000)\n        )\n\n        self.conv_architecture = self.conv_architectures[architecture_identifier]\n        self.conv_layers = nn.Sequential()\n        \n        for options in self.conv_architecture:\n            if isinstance(options, tuple):\n                self.conv_layers.append(nn.Conv2d(*options, padding=\"same\", stride=1))\n                self.conv_layers.append(nn.ReLU())\n            elif options == \"lrn\":\n                self.conv_layers.append(nn.LocalResponseNorm(5, alpha=1e-4, beta=0.75, k=2))\n            elif options == \"max\":\n                self.conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n        \n        self.model = nn.Sequential(OrderedDict([\n            (\"feature_extractor\", self.conv_layers),\n            (\"classifier\", self.fully_connected)\n        ]))\n\n    def forward(self, input):\n        return self.model(input)\n    \n\nLet’s take a look at model A with LocalResponseNormalization:\n\nVGG(\"A_LRN\").model\n\nc:\\Users\\edesa\\Desktop\\ML Research\\env\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n\n\nSequential(\n  (feature_extractor): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (1): ReLU()\n    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (5): ReLU()\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (8): ReLU()\n    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (10): ReLU()\n    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (13): ReLU()\n    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (15): ReLU()\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (18): ReLU()\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (20): ReLU()\n    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): LazyLinear(in_features=0, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=4096, out_features=4069, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\n\n\nTraining\n\nbatch_size = 256\nmomentum = 0.9\nweight_decay = 5e-4\n\n\ndataset = datasets.Caltech256(\"data/\",\n                                download=True,\n                                transform=transforms.Compose([\n                                    gray_to_rgb,\n                                    transforms.ToTensor(),\n                                    transforms.Resize((512, 512), antialias=False),\n                                    transforms.RandomCrop(size=(224, 224)),\n                                    transforms.RandomHorizontalFlip(),\n                                    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                                    transforms.Normalize(\n                                        mean=[0.485, 0.456, 0.406],\n                                        std=[0.229, 0.224, 0.225]\n                                    ),\n                                ]))\ndataloader = DataLoader(dataset,\n                        batch_size=batch_size,\n                        shuffle=True)\n\nFiles already downloaded and verified\n\n\n\n\n\nAfter Image Augmentation\n\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(momentum=momentum, weight_decay=weight_decay)\n\n\ndef train_one_epoch(epoch_index, tb_writer):\n    running_loss = 0.\n    last_loss = 0.\n\n    # Here, we use enumerate(training_loader) instead of\n    # iter(training_loader) so that we can track the batch\n    # index and do some intra-epoch reporting\n    for i, data in enumerate(dataloader):\n        # Every data instance is an input + label pair\n        inputs, labels = data\n\n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Make predictions for this batch\n        outputs = model(inputs)\n\n        # Compute the loss and its gradients\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n\n        # Adjust learning weights\n        optimizer.step()\n\n        # Gather data and report\n        running_loss += loss.item()\n        if i % 1000 == 999:\n            last_loss = running_loss / 1000 # loss per batch\n            print('  batch {} loss: {}'.format(i + 1, last_loss))\n            tb_x = epoch_index * len(training_loader) + i + 1\n            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n            running_loss = 0.\n\n    return last_loss\n\n\n\nConclusion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "VGG Paper Implementation from Scratch\n\n\n\n\n\n\n\nIN PROGRESS\n\n\nDeep Learning\n\n\nPyTorch\n\n\nVGG\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nAras Edeş\n\n\n\n\n\n\nNo matching items"
  }
]